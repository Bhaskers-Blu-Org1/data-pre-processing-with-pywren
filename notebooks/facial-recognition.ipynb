{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"blue\"><b>\n",
    "          Face Recognition Deep Learning with PyWren over IBM Cloud Functions</b></font></font></th>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains steps and code to demonstrate how serverless computing can provide great benefit for AI data preprocessing. We demonstrate Face Recognition deep learning over Watson Machine Learning service, while letting IBM Cloud Function to do the data preparation phase. As we will show this makes an entire process up to 50 times faster comparing to running the same code without leveraging serverless computing.\n",
    "\n",
    "Our notebook is based on a blog <a href=\"https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8\" target=\"_blank\" rel=\"noopener no referrer\">Building a Facial Recognition Pipeline with Deep Learning in Tensorflow </a> written by Cole Murray who kindly allowed us to use code and text from his blog.\n",
    "\n",
    "This notebook introduces commands for getting data, training_definition persistance to Watson Machine Learning repository, model training, deployment and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses \n",
    "\n",
    "- Python 3 \n",
    "- <a href=\"https://dataplatform.cloud.ibm.com/docs/content/analyze-data/environments-parent.html\" target=\"_blank\" rel=\"noopener no referrer\">Watson Studio environments.</a>\n",
    "- IBM Cloud Functions\n",
    "- <a href=\"https://github.com/pywren/pywren-ibm-cloud\" target=\"_blank\" rel=\"noopener no referrer\">PyWren for IBM Cloud</a>\n",
    "\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "\n",
    "-  Work with Watson Machine Learning experiments to train Deep Learning models (Tensorflow)\n",
    "-  Save trained models in the Watson Machine Learning repository\n",
    "-  Deploy a trained model online and score\n",
    "-  How IBM Cloud Functions can be used for data preparation phase\n",
    "-  Value of PyWren for IBM Cloud\n",
    "\n",
    "\n",
    "## Contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## <span style=\"color:blue\">1. Set up related IBM Cloud Services</span>\n",
    "\n",
    "Before you use the sample code in this notebook, you must setup Watson Machine Learning Service, IBM Cloud Object Storage and IBM Cloud Functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create Watson Machine Learning Service\n",
    "\n",
    "Create a <a href=\"https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance is <a href=\"https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create IBM Cloud Object Storage\n",
    "\n",
    "Create a <a href=\"https://console.bluemix.net/catalog/infrastructure/cloud-object-storage\" target=\"_blank\" rel=\"noopener no referrer\">Cloud Object Storage (COS)</a> instance (a lite plan is offered and information about how to order storage is <a href=\"https://console.bluemix.net/docs/services/cloud-object-storage/basics/order-storage.html#order-storage\" target=\"_blank\" rel=\"noopener no referrer\">here</a>). <br/>**Note: When using Watson Studio, you already have a COS instance associated with the project you are running the notebook in.**\n",
    "\n",
    "- Create new credentials with HMAC: \n",
    "    - Go to your COS dashboard.\n",
    "    - In the **Service credentials** tab, click **New Credential+**.\n",
    "    - Add the inline configuration parameter: {\"HMAC\":true}, click **Add**. (For more information, see <a href=\"https://console.bluemix.net/docs/services/cloud-object-storage/hmac/credentials.html#using-hmac-credentials\" target=\"_blank\" rel=\"noopener no referrer\">HMAC</a>.)\n",
    "\n",
    "    This configuration parameter adds the following section to the instance credentials, (for use later in this notebook):\n",
    "    ```\n",
    "      \"cos_hmac_keys\": {\n",
    "            \"access_key_id\": \"-------\",\n",
    "            \"secret_access_key\": \"-------\"\n",
    "       }\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create IBM Cloud Functions account\n",
    "Setup IBM Cloud Functions account as described here. Please follow all the steps and make sure you can run \"Hello World\" example based on Python code. This will assure your Cloud Functions service is running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> 2. Dependencies installation </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the needed libraries for the Face Recognition. \n",
    "\"dlib\" dependency need to be installed via new environment. Create new environment based on Python 3.5 and add dependency in the customizaion section, as follows\n",
    "\n",
    "    dependencies:\n",
    "     - dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!curl -fsSL \"https://git.io/fhe9X\" | sh\n",
    "try:\n",
    "    import pywren_ibm_cloud as pywren\n",
    "except:\n",
    "    !curl -fsSL \"https://git.io/fhe9X\" | sh\n",
    "    import pywren_ibm_cloud as pywren\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    !pip install --user opencv-contrib-python\n",
    "try:\n",
    "    from openface.align_dlib import AlignDlib\n",
    "except:    \n",
    "    !git clone https://github.com/cmusatyalab/openface.git\n",
    "    !cd openface ; python setup.py install\n",
    "\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">3. Configuration </span>\n",
    "This section explains how to configure services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 COS Connection\n",
    "You need obtain both crednetials to the Cloud Functions and COS\n",
    "\n",
    "You can find COS credentials in your COS instance dashboard under the Service credentials tab.\n",
    "Note: the HMAC key, described in set up the environment is included in these credentials.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_credentials = {\n",
    "  \"apikey\": \"***\",\n",
    "  \"cos_hmac_keys\": {\n",
    "    \"access_key_id\": \"***\",\n",
    "    \"secret_access_key\": \"***\"\n",
    "  },\n",
    "  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:cloud-object-storage:global:a/07a95aa44e6124e8b320b70cf88033fa:876e5285-4bef-4cf3-a89b-595e19648c7c::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-19a79dae-6a58-4b4f-878f-6839b711523f\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/07a95aa44e6124e8b320b70cf88033fa::serviceid:ServiceId-3f2cccee-61ec-4147-8732-9f58479ba26a\",\n",
    "  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/07a95aa44e6124e8b320b70cf88033fa:876e5285-4bef-4cf3-a89b-595e19648c7c::\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the endpoint.\n",
    "\n",
    "To do this, go to the **Endpoint** tab in the COS instance's dashboard to get the endpoint information, then enter it in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define endpoint information.\n",
    "service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also need the IBM Cloud authorization endpoint to be able to create COS resource object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the authorization endpoint.\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the boto library. This library allows Python developers to manage Cloud Object Storage (COS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** If `ibm_boto3` is not preinstalled in you environment, run the following command to install it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command if ibm_boto3 is not installed.\n",
    "%!pip install ibm-cos-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the boto library.\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Boto resource to be able to write data to COS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a COS resource.\n",
    "cos = ibm_boto3.resource('s3',\n",
    "                         ibm_api_key_id=cos_credentials['apikey'],\n",
    "                         ibm_service_instance_id=cos_credentials['resource_instance_id'],\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 IBM Cloud Functions setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain api key and endpoint to the IBM Cloud Functions service. Navigate the \"API Key\" menu and copy namespace, host and key. Make sure to add \"https://\" to the host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          'ibm_cf':  {'endpoint': 'https://us-east.functions.cloud.ibm.com', \n",
    "                      'namespace': '<NAMESPACE>', \n",
    "                      'api_key': '<API KEY>'}, \n",
    "          'ibm_cos': {'endpoint': 'https://s3-api.us-geo.objectstorage.softlayer.net', \n",
    "                      'api_key' : '<API KEY>'},\n",
    "           'pywren' : {'storage_bucket' : '<IBM COS BUCKET>'}\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyWren engine requires it's server side component to be deplpoyed in advane. This step creates a new IBM Cloud Function function with PyWren server side runtime. This action will be used internally by PyWren during execution phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywren_ibm_cloud.deployutil import clone_runtime\n",
    "clone_runtime('cactusone/pywren:3.5', config, 'pywren-ibm-cloud-master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Data using Dlib and Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preparing the Data\n",
    "\n",
    "You’ll use the LFW (Labeled Faces in the Wild) dataset as training data. Below are instructions how you can upload this dataset into your private COS bucket. Alternatively you may replace this with your dataset by following the same structure.\n",
    "\n",
    "\n",
    "     Directory Structure\n",
    "     ├── Tyra_Banks\n",
    "     │ ├── Tyra_Banks_0001.jpg\n",
    "     │ └── Tyra_Banks_0002.jpg\n",
    "     ├── Tyron_Garner\n",
    "     │ ├── Tyron_Garner_0001.jpg\n",
    "     │ └── Tyron_Garner_0002.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a COS bucket, which you will use to store the input data.\n",
    "\n",
    "**Note:** The bucket names must be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_uid = str(uuid4())\n",
    "BUCKET = 'face-recognition-' + bucket_uid\n",
    "\n",
    "if not cos.Bucket(BUCKET) in cos.buckets.all():\n",
    "    print('Creating bucket \"{}\"...'.format(BUCKET))\n",
    "    try:\n",
    "        cos.create_bucket(Bucket=BUCKET)\n",
    "    except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n",
    "    print('Error: {}.'.format(e.response['Error']['Message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have 1 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display a list of created buckets.\n",
    "print(list(cos.buckets.all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step copies images from Labeled Faces in the Wild into your COS bucket.\n",
    "We demonstrate with small data set of 14MB. If you wish to you use entire data set, then use \n",
    "\n",
    "    url = \"http://vis-www.cs.umass.edu/lfw/lfw.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import io\n",
    " \n",
    "\n",
    "def extractFromStream(url, cos, target_prefix = None):\n",
    "    ftp_stream = urllib.request.urlopen(url)\n",
    "    tarfile_like_object = io.BytesIO(ftp_stream.read())\n",
    "    TarFile_object = tarfile.open(fileobj=tarfile_like_object)\n",
    "    for member in TarFile_object:\n",
    "        if member.isdir() == False:\n",
    "            member_like_object = TarFile_object.extractfile(member)\n",
    "            key = target_prefix\n",
    "            if target_prefix is not None:\n",
    "                key = target_prefix + '/' + member.name\n",
    "            cos.Object(BUCKET, key).put(Body=member_like_object.read())\n",
    "            break\n",
    "\n",
    " \n",
    " \n",
    "url = \"http://vis-www.cs.umass.edu/lfw/lfw-a.tgz\"\n",
    "extractFromStream(url, cos, \"rawimages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket_name in buckets:\n",
    "    print(bucket_name)\n",
    "    bucket_obj = cos.Bucket(bucket_name)\n",
    "    for obj in bucket_obj.objects.all():\n",
    "        print(\"  File: {}, {:4.2f}kB\".format(obj.key, obj.size/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data preprocessing with serveless\n",
    "\n",
    "Below, you’ll pre-process the images before passing them into the FaceNet model. Image pre-processing in a facial recognition context typically solves a few problems. These problems range from lighting differences, occlusion, alignment, segmentation. Below, you’ll address segmentation and alignment.\n",
    "First, you’ll solve the segmentation problem by finding the largest face in an image. This is useful as our training data does not have to be cropped for a face ahead of time.\n",
    "Second, you’ll solve alignment. In photographs, it is common for a face to not be perfectly center aligned with the image. To standardize input, you’ll apply a transform to center all images based on the location of eyes and bottom lip.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Detect, Crop & Align with Dlib\n",
    "\n",
    "Upload dlib’s face landmark predictor into your COS bucket. You’ll use this face landmark predictor to find the location of the inner eyes and bottom lips of a face in an image. These coordinates will be used to center align the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n",
    "extractFromStream(url, cos, 'predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Preprocessing with IBM Cloud Functions\n",
    "Next, you’ll create a preprocessor for your dataset. This file will read each image into memory, attempt to find the largest face, center align, and write the file to output. If a face cannot be found in the image, logging will be displayed to console with the filename.\n",
    "As each image can be processed independently, python’s multiprocessing is used to process an image on each available cpu core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import pywren_ibm_cloud as pywren\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "from openface.align_dlib import AlignDlib\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "temp_dir = '/tmp'\n",
    "\n",
    "def preprocess_image(bucket, key, data_stream, ibm_cos):\n",
    "    \"\"\"\n",
    "    Detect face, align and crop :param input_path. Write output to :param output_path\n",
    "    :param bucket: COS bucket\n",
    "    :param key: COS key (object name ) - may contain delimiters\n",
    "    :param storage_handler: can be used to read / write data from / into COS\n",
    "    \"\"\"\n",
    "    crop_dim = 180\n",
    "    print(\"Process bucket {} key {}\".format(bucket, key))    \n",
    "    # key of the form /subdir1/../subdirN/file_name\n",
    "    key_components = key.split('/')\n",
    "    file_name = key_components[len(key_components)-1]\n",
    "    input_path = temp_dir + '/' + file_name\n",
    "    if not os.path.exists(temp_dir + '/' + 'output'):\n",
    "        os.makedirs(temp_dir + '/' +'output')\n",
    "    output_path = temp_dir + '/' +'output/'  + file_name\n",
    "    with open(input_path, 'wb') as localfile:\n",
    "        shutil.copyfileobj(data_stream, localfile)\n",
    "    exists = os.path.isfile(temp_dir + '/' +'shape_predictor_68_face_landmarks')\n",
    "    if exists:\n",
    "        pass;\n",
    "    else:\n",
    "        res = ibm_cos.get_object(Bucket = bucket, Key = 'predictor/shape_predictor_68_face_landmarks.dat')\n",
    "        with open(temp_dir + '/' +'shape_predictor_68_face_landmarks', 'wb') as localfile:\n",
    "            shutil.copyfileobj(res['Body'], localfile)\n",
    "    align_dlib = AlignDlib(temp_dir + '/' +'shape_predictor_68_face_landmarks')\n",
    "    image = _process_image(input_path, crop_dim, align_dlib)\n",
    "    if image is not None:\n",
    "        print('Writing processed file: {}'.format(output_path))\n",
    "        cv2.imwrite(output_path, image)\n",
    "        f = open(output_path, \"rb\")\n",
    "        processed_image_path = os.path.join('output',key)\n",
    "        ibm_cos.put_object(Bucket = bucket, Key = processed_image_path, Body = f)\n",
    "        os.remove(output_path)\n",
    "    else:\n",
    "        print(\"Skipping filename: {}\".format(input_path))\n",
    "    os.remove(input_path)\n",
    "\n",
    "def _process_image(filename, crop_dim, align_dlib):\n",
    "    image = None\n",
    "    aligned_image = None\n",
    "    image = _buffer_image(filename)\n",
    "    if image is not None:\n",
    "        aligned_image = _align_image(image, crop_dim, align_dlib)\n",
    "    else:\n",
    "        raise IOError('Error buffering image: {}'.format(filename))\n",
    "    return aligned_image\n",
    "\n",
    "def _buffer_image(filename):\n",
    "    logger.debug('Reading image: {}'.format(filename))\n",
    "    image = cv2.imread(filename, )\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def _align_image(image, crop_dim, align_dlib):\n",
    "    bb = align_dlib.getLargestFaceBoundingBox(image)\n",
    "    aligned = align_dlib.align(crop_dim, image, bb, landmarkIndices=AlignDlib.INNER_EYES_AND_BOTTOM_LIP)\n",
    "    if aligned is not None:\n",
    "        aligned = cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB)\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Getting Results\n",
    "\n",
    "Now that you’ve created a pipeline, time to get results. As the script supports parallelism, you will see increased performance by running with multiple cores. You’ll need to run the preprocessor in the docker environment to have access to the installed libraries.\n",
    "Below, you’ll mount your project directory as a volume inside the docker container and run the preprocessing script on your input data. The results will be written to a directory specified with command line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_images = BUCKET + '/rawimages'    \n",
    "pw = pywren.ibm_cf_executor(config=config, runtime='pywren-dlib-runtime_3.5')    \n",
    "pw.map(preprocess_image, raw_images)\n",
    "results = pw.get_result()\n",
    "print(\"Execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "Using Dlib, you detected the largest face in an image and aligned the center of the face by the inner eyes and bottom lip. This alignment is a method for standardizing each image for use as feature input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Work with the WML service instance\n",
    "\n",
    "From this point you need to adapt flows from the original blog to use Watson ML. Below is copy-paste from the original blog to easy the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow section Creating Embeddings in Tensorflow from https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8\n",
    "\n",
    "Assume that data was prepocessed and stored in COS.\n",
    "\n",
    "    bucket name : gilvdata\n",
    "\n",
    "all images stored under \n",
    "    output/lfw/test/images/*\n",
    "\n",
    "for example\n",
    "\n",
    "    output/lfw/test/images/AJ_Cook/AJ_Cook_0001.jpg\n",
    "    output/lfw/test/images/AJ_Lamas/AJ_Lamas_0001.jpg\n",
    "    output/lfw/test/images/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\n",
    "    output/lfw/test/images/Aaron_Guiel/Aaron_Guiel_0001.jpg\n",
    "    output/lfw/test/images/Aaron_Patterson/Aaron_Patterson_0001.jpg\n",
    "    output/lfw/test/images/Aaron_Peirsol/Aaron_Peirsol_0001.jpg\n",
    "    output/lfw/test/images/Aaron_Peirsol/Aaron_Peirsol_0002.jpg\n",
    "\n",
    "Model for dlib that was used to pre-process image is stored under\n",
    "\n",
    "    lfw/model/shape_predictor_68_face_landmarks.dat\n",
    "\n",
    "(this doesn't seems needed for the TensorFlow part of the blog )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings in Tensorflow\n",
    "\n",
    "Now that you’ve preprocessed the data, you’ll generate vector embeddings of each identity. These embeddings can then be used as input to a classification, regression or clustering task.\n",
    "\n",
    "### Download Weights\n",
    "\n",
    "You’ll use the Inception Resnet V1 as your convolutional neural network. First, create a file to download the weights to the model.\n",
    "By using pre-trained weights, you are able to apply transfer learning to a new dataset, in this tutorial the LFW dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "\"\"\"\n",
    "This file is copied from:\n",
    "https://github.com/davidsandberg/facenet/blob/master/src/download_and_extract_model.py\n",
    "\"\"\"\n",
    "\n",
    "model_dict = {\n",
    "    '20170511-185253': '0B5MzpY9kBtDVOTVnU3NIaUdySFE'\n",
    "}\n",
    "\n",
    "\n",
    "def download_and_extract_model(model_name, data_dir):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    file_id = model_dict[model_name]\n",
    "    destination = os.path.join(data_dir, model_name + '.zip')\n",
    "    if not os.path.exists(destination):\n",
    "        print('Downloading model to %s' % destination)\n",
    "        download_file_from_google_drive(file_id, destination)\n",
    "        with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
    "            print('Extracting model to %s' % data_dir)\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://drive.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params={'id': file_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': file_id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser.add_argument('--model-dir', type=str, action='store', dest='model_dir',\n",
    "                        help='Path to model protobuf graph')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    download_and_extract_model('20170511-185253', args.model_dir)\n",
    "\n",
    "'''\n",
    "$ docker run -v $PWD:/medium-facenet-tutorial \\\n",
    "-e PYTHONPATH=$PYTHONPATH:/medium-facenet-tutorial \\\n",
    "-it colemurray/medium-facenet-tutorial python3 /medium-facenet-tutorial/medium_facenet_tutorial/download_and_extract_model.py \\\n",
    "--model-dir /medium-facenet-tutorial/etc\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings\n",
    "\n",
    "Below, you’ll utilize Tensorflow’s queue api to load the preprocessed images in parallel. By using queues, images can be loaded in parallel using multi-threading. When using a GPU, this allows image preprocessing to be performed on CPU, while matrix multiplication is performed on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def read_data(image_paths, label_list, image_size, batch_size, max_nrof_epochs, num_threads, shuffle, random_flip,\n",
    "              random_brightness, random_contrast):\n",
    "    \"\"\"\n",
    "    Creates Tensorflow Queue to batch load images. Applies transformations to images as they are loaded.\n",
    "    :param random_brightness: \n",
    "    :param random_flip: \n",
    "    :param image_paths: image paths to load\n",
    "    :param label_list: class labels for image paths\n",
    "    :param image_size: size to resize images to\n",
    "    :param batch_size: num of images to load in batch\n",
    "    :param max_nrof_epochs: total number of epochs to read through image list\n",
    "    :param num_threads: num threads to use\n",
    "    :param shuffle: Shuffle images\n",
    "    :param random_flip: Random Flip image\n",
    "    :param random_brightness: Apply random brightness transform to image\n",
    "    :param random_contrast: Apply random contrast transform to image\n",
    "    :return: images and labels of batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    images = ops.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "    labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\n",
    "\n",
    "    # Makes an input queue\n",
    "    input_queue = tf.train.slice_input_producer((images, labels),\n",
    "                                                num_epochs=max_nrof_epochs, shuffle=shuffle, )\n",
    "\n",
    "    images_labels = []\n",
    "    imgs = []\n",
    "    lbls = []\n",
    "    for _ in range(num_threads):\n",
    "        image, label = read_image_from_disk(filename_to_label_tuple=input_queue)\n",
    "        image = tf.random_crop(image, size=[image_size, image_size, 3])\n",
    "        image.set_shape((image_size, image_size, 3))\n",
    "        image = tf.image.per_image_standardization(image)\n",
    "\n",
    "        if random_flip:\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "        if random_brightness:\n",
    "            image = tf.image.random_brightness(image, max_delta=0.3)\n",
    "\n",
    "        if random_contrast:\n",
    "            image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
    "\n",
    "        imgs.append(image)\n",
    "        lbls.append(label)\n",
    "        images_labels.append([image, label])\n",
    "\n",
    "    image_batch, label_batch = tf.train.batch_join(images_labels,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   capacity=4 * num_threads,\n",
    "                                                   enqueue_many=False,\n",
    "                                                   allow_smaller_final_batch=True)\n",
    "    return image_batch, label_batch\n",
    "\n",
    "\n",
    "def read_image_from_disk(filename_to_label_tuple):\n",
    "    \"\"\"\n",
    "    Consumes input tensor and loads image\n",
    "    :param filename_to_label_tuple: \n",
    "    :type filename_to_label_tuple: list\n",
    "    :return: tuple of image and label\n",
    "    \"\"\"\n",
    "    label = filename_to_label_tuple[1]\n",
    "    file_contents = tf.read_file(filename_to_label_tuple[0])\n",
    "    example = tf.image.decode_jpeg(file_contents, channels=3)\n",
    "    return example, label\n",
    "\n",
    "\n",
    "def get_image_paths_and_labels(dataset):\n",
    "    image_paths_flat = []\n",
    "    labels_flat = []\n",
    "    for i in range(int(len(dataset))):\n",
    "        image_paths_flat += dataset[i].image_paths\n",
    "        labels_flat += [i] * len(dataset[i].image_paths)\n",
    "    return image_paths_flat, labels_flat\n",
    "\n",
    "\n",
    "def get_dataset(input_directory):\n",
    "    dataset = []\n",
    "\n",
    "    classes = os.listdir(input_directory)\n",
    "    classes.sort()\n",
    "    nrof_classes = len(classes)\n",
    "    for i in range(nrof_classes):\n",
    "        class_name = classes[i]\n",
    "        facedir = os.path.join(input_directory, class_name)\n",
    "        if os.path.isdir(facedir):\n",
    "            images = os.listdir(facedir)\n",
    "            image_paths = [os.path.join(facedir, img) for img in images]\n",
    "            dataset.append(ImageClass(class_name, image_paths))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def filter_dataset(dataset, min_images_per_label=10):\n",
    "    filtered_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        if len(dataset[i].image_paths) < min_images_per_label:\n",
    "            logger.info('Skipping class: {}'.format(dataset[i].name))\n",
    "            continue\n",
    "        else:\n",
    "            filtered_dataset.append(dataset[i])\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, split_ratio=0.8):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    min_nrof_images = 2\n",
    "    for cls in dataset:\n",
    "        paths = cls.image_paths\n",
    "        np.random.shuffle(paths)\n",
    "        split = int(round(len(paths) * split_ratio))\n",
    "        if split < min_nrof_images:\n",
    "            continue  # Not enough images for test set. Skip class...\n",
    "        train_set.append(ImageClass(cls.name, paths[0:split]))\n",
    "        test_set.append(ImageClass(cls.name, paths[split:-1]))\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "class ImageClass():\n",
    "    def __init__(self, name, image_paths):\n",
    "        self.name = name\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name + ', ' + str(len(self.image_paths)) + ' images'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Classifier\n",
    "With the input queue squared away, you’ll move on to creating the embeddings.\n",
    "First, you’ll load the images from the queue you created. While training, you’ll apply preprocessing to the image. This preprocessing will add random transformations to the image, creating more images to train on.\n",
    "These images will be fed in a batch size of 128 into the model. This model will return a 128 dimensional embedding for each image, returning a 128 x 128 matrix for each batch.\n",
    "After these embeddings are created, you’ll use them as feature inputs into a scikit-learn’s SVM classifier to train on each identity. Identities with less than 10 images will be dropped. This parameter is tunable from command-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "from lfw_input import filter_dataset, split_dataset, get_dataset\n",
    "from medium_facenet_tutorial import lfw_input\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main(input_directory, model_path, classifier_output_path, batch_size, num_threads, num_epochs,\n",
    "         min_images_per_labels, split_ratio, is_train=True):\n",
    "    \"\"\"\n",
    "    Loads images from :param input_dir, creates embeddings using a model defined at :param model_path, and trains\n",
    "     a classifier outputted to :param output_path\n",
    "     \n",
    "    :param input_directory: Path to directory containing pre-processed images\n",
    "    :param model_path: Path to protobuf graph file for facenet model\n",
    "    :param classifier_output_path: Path to write pickled classifier\n",
    "    :param batch_size: Batch size to create embeddings\n",
    "    :param num_threads: Number of threads to utilize for queuing\n",
    "    :param num_epochs: Number of epochs for each image\n",
    "    :param min_images_per_labels: Minimum number of images per class\n",
    "    :param split_ratio: Ratio to split train/test dataset\n",
    "    :param is_train: bool denoting if training or evaluate\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as sess:\n",
    "        train_set, test_set = _get_test_and_train_set(input_directory, min_num_images_per_label=min_images_per_labels,\n",
    "                                                      split_ratio=split_ratio)\n",
    "        if is_train:\n",
    "            images, labels, class_names = _load_images_and_labels(train_set, image_size=160, batch_size=batch_size,\n",
    "                                                                  num_threads=num_threads, num_epochs=num_epochs,\n",
    "                                                                  random_flip=True, random_brightness=True,\n",
    "                                                                  random_contrast=True)\n",
    "        else:\n",
    "            images, labels, class_names = _load_images_and_labels(test_set, image_size=160, batch_size=batch_size,\n",
    "                                                                  num_threads=num_threads, num_epochs=1)\n",
    "\n",
    "        _load_model(model_filepath=model_path)\n",
    "\n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        sess.run(init_op)\n",
    "\n",
    "        images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "        embedding_layer = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "        emb_array, label_array = _create_embeddings(embedding_layer, images, labels, images_placeholder,\n",
    "                                                    phase_train_placeholder, sess)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads=threads)\n",
    "        logger.info('Created {} embeddings'.format(len(emb_array)))\n",
    "\n",
    "        classifier_filename = classifier_output_path\n",
    "\n",
    "        if is_train:\n",
    "            _train_and_save_classifier(emb_array, label_array, class_names, classifier_filename)\n",
    "        else:\n",
    "            _evaluate_classifier(emb_array, label_array, classifier_filename)\n",
    "\n",
    "        logger.info('Completed in {} seconds'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "def _get_test_and_train_set(input_dir, min_num_images_per_label, split_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Load train and test dataset. Classes with < :param min_num_images_per_label will be filtered out.\n",
    "    :param input_dir: \n",
    "    :param min_num_images_per_label: \n",
    "    :param split_ratio: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    dataset = get_dataset(input_dir)\n",
    "    dataset = filter_dataset(dataset, min_images_per_label=min_num_images_per_label)\n",
    "    train_set, test_set = split_dataset(dataset, split_ratio=split_ratio)\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def _load_images_and_labels(dataset, image_size, batch_size, num_threads, num_epochs, random_flip=False,\n",
    "                            random_brightness=False, random_contrast=False):\n",
    "    class_names = [cls.name for cls in dataset]\n",
    "    image_paths, labels = lfw_input.get_image_paths_and_labels(dataset)\n",
    "    images, labels = lfw_input.read_data(image_paths, labels, image_size, batch_size, num_epochs, num_threads,\n",
    "                                         shuffle=False, random_flip=random_flip, random_brightness=random_brightness,\n",
    "                                         random_contrast=random_contrast)\n",
    "    return images, labels, class_names\n",
    "\n",
    "\n",
    "def _load_model(model_filepath):\n",
    "    \"\"\"\n",
    "    Load frozen protobuf graph\n",
    "    :param model_filepath: Path to protobuf graph\n",
    "    :type model_filepath: str\n",
    "    \"\"\"\n",
    "    model_exp = os.path.expanduser(model_filepath)\n",
    "    if os.path.isfile(model_exp):\n",
    "        logging.info('Model filename: %s' % model_exp)\n",
    "        with gfile.FastGFile(model_exp, 'rb') as f:\n",
    "            graph_def = tf.GraphDef()\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "    else:\n",
    "        logger.error('Missing model file. Exiting')\n",
    "        sys.exit(-1)\n",
    "\n",
    "\n",
    "def _create_embeddings(embedding_layer, images, labels, images_placeholder, phase_train_placeholder, sess):\n",
    "    \"\"\"\n",
    "    Uses model to generate embeddings from :param images.\n",
    "    :param embedding_layer: \n",
    "    :param images: \n",
    "    :param labels: \n",
    "    :param images_placeholder: \n",
    "    :param phase_train_placeholder: \n",
    "    :param sess: \n",
    "    :return: (tuple): image embeddings and labels\n",
    "    \"\"\"\n",
    "    emb_array = None\n",
    "    label_array = None\n",
    "    try:\n",
    "        i = 0\n",
    "        while True:\n",
    "            batch_images, batch_labels = sess.run([images, labels])\n",
    "            logger.info('Processing iteration {} batch of size: {}'.format(i, len(batch_labels)))\n",
    "            emb = sess.run(embedding_layer,\n",
    "                           feed_dict={images_placeholder: batch_images, phase_train_placeholder: False})\n",
    "\n",
    "            emb_array = np.concatenate([emb_array, emb]) if emb_array is not None else emb\n",
    "            label_array = np.concatenate([label_array, batch_labels]) if label_array is not None else batch_labels\n",
    "            i += 1\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    return emb_array, label_array\n",
    "\n",
    "\n",
    "def _train_and_save_classifier(emb_array, label_array, class_names, classifier_filename_exp):\n",
    "    logger.info('Training Classifier')\n",
    "    model = SVC(kernel='linear', probability=True, verbose=False)\n",
    "    model.fit(emb_array, label_array)\n",
    "\n",
    "    with open(classifier_filename_exp, 'wb') as outfile:\n",
    "        pickle.dump((model, class_names), outfile)\n",
    "    logging.info('Saved classifier model to file \"%s\"' % classifier_filename_exp)\n",
    "\n",
    "\n",
    "def _evaluate_classifier(emb_array, label_array, classifier_filename):\n",
    "    logger.info('Evaluating classifier on {} images'.format(len(emb_array)))\n",
    "    if not os.path.exists(classifier_filename):\n",
    "        raise ValueError('Pickled classifier not found, have you trained first?')\n",
    "\n",
    "    with open(classifier_filename, 'rb') as f:\n",
    "        model, class_names = pickle.load(f)\n",
    "\n",
    "        predictions = model.predict_proba(emb_array, )\n",
    "        best_class_indices = np.argmax(predictions, axis=1)\n",
    "        best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n",
    "\n",
    "        for i in range(len(best_class_indices)):\n",
    "            print('%4d  %s: %.3f' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n",
    "\n",
    "        accuracy = np.mean(np.equal(best_class_indices, label_array))\n",
    "        print('Accuracy: %.3f' % accuracy)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser.add_argument('--model-path', type=str, action='store', dest='model_path',\n",
    "                        help='Path to model protobuf graph')\n",
    "    parser.add_argument('--input-dir', type=str, action='store', dest='input_dir',\n",
    "                        help='Input path of data to train on')\n",
    "    parser.add_argument('--batch-size', type=int, action='store', dest='batch_size',\n",
    "                        help='Input path of data to train on', default=128)\n",
    "    parser.add_argument('--num-threads', type=int, action='store', dest='num_threads', default=16,\n",
    "                        help='Number of threads to utilize for queue')\n",
    "    parser.add_argument('--num-epochs', type=int, action='store', dest='num_epochs', default=3,\n",
    "                        help='Path to output trained classifier model')\n",
    "    parser.add_argument('--split-ratio', type=float, action='store', dest='split_ratio', default=0.7,\n",
    "                        help='Ratio to split train/test dataset')\n",
    "    parser.add_argument('--min-num-images-per-class', type=int, action='store', default=10,\n",
    "                        dest='min_images_per_class', help='Minimum number of images per class')\n",
    "    parser.add_argument('--classifier-path', type=str, action='store', dest='classifier_path',\n",
    "                        help='Path to output trained classifier model')\n",
    "    parser.add_argument('--is-train', action='store_true', dest='is_train', default=False,\n",
    "                        help='Flag to determine if train or evaluate')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(input_directory=args.input_dir, model_path=args.model_path, classifier_output_path=args.classifier_path,\n",
    "         batch_size=args.batch_size, num_threads=args.num_threads, num_epochs=args.num_epochs,\n",
    "         min_images_per_labels=args.min_images_per_class, split_ratio=args.split_ratio, is_train=args.is_train)\n",
    "'''\n",
    "$ docker run -v $PWD:/medium-facenet-tutorial \\\n",
    "-e PYTHONPATH=$PYTHONPATH:/medium-facenet-tutorial \\\n",
    "-it colemurray/medium-facenet-tutorial \\\n",
    "python3 /medium-facenet-tutorial/medium_facenet_tutorial/train_classifier.py \\\n",
    "--input-dir /medium-facenet-tutorial/output/intermediate \\\n",
    "--model-path /medium-facenet-tutorial/etc/20170511-185253/20170511-185253.pb \\\n",
    "--classifier-path /medium-facenet-tutorial/output/classifier.pkl \\\n",
    "--num-threads 16 \\\n",
    "--num-epochs 25 \\\n",
    "--min-num-images-per-class 10 \\\n",
    "--is-train \n",
    "# ~16 mins to complete on MBP\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Results\n",
    "Now that you’ve trained the classifier, you’ll feed it new images it has not trained on. You’ll remove the is_train flag from the previous command to evaluate your results.\n",
    "\n",
    "    docker run -v $PWD:/$(basename $PWD) \\\n",
    "    -e PYTHONPATH=$PYTHONPATH:/medium-facenet-tutorial \\\n",
    "    -it colemurray/medium-facenet-tutorial \\\n",
    "    python3 /medium-facenet-tutorial/medium_facenet_tutorial/train_classifier.py \\\n",
    "    --input-dir /medium-facenet-tutorial/output/intermediate \\\n",
    "    --model-path /medium-facenet-tutorial/etc/20170511-185253/20170511-185253.pb \\\n",
    "    --classifier-path /medium-facenet-tutorial/output/classifier.pkl \\\n",
    "    --num-threads 16 \\\n",
    "    --num-epochs 5 \\\n",
    "    --min-num-images-per-class 10\n",
    "    \n",
    "After inference is on each image is complete, you’ll see results printed to console. At 5 epochs, you’ll see ~85.0% accuracy. Training @ 25 epochs gave results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "**Gil Vernik? Who else? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2019 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
